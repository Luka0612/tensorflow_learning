{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始矩阵:\n[[-0.4090131  -0.2457109   0.7692419  -0.10311802]\n [-0.18510456  0.5100287   1.9183942  -0.8198766 ]\n [-1.02769    -0.68557066 -0.18391953  0.72068346]\n [ 0.2200638  -0.29648167 -1.7098439  -1.2139964 ]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "# 默认Tensorflow会话\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 对数据进行准备\n",
    "# 产生一个4x4的矩阵，满足均值为0，标准差为1的正态分布\n",
    "a = tf.Variable(tf.random_normal([4, 4], mean=0.0, stddev=1.0))\n",
    "\n",
    "# 对所有变量进行初始化，这里对a进行初始化\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# 输出原始的a的值\n",
    "print\"原始矩阵:\\n\", a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anwser 1: 0.05215356\nanwser 2: 0.9677046\nsigmoid函数激活后的矩阵:\n[[0.28072977 0.542763   0.4840443  0.091187  ]\n [0.57963765 0.33234805 0.73203653 0.898169  ]\n [0.38909778 0.6560786  0.5287672  0.58028483]\n [0.25249916 0.2798983  0.08161165 0.4413188 ]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# sigmoid激活函数：\\frac{1}{1 + e^{-x}}\n",
    "# 优点：输出限制在0，1之间，如LSTM的门控就必须使用sigmoid\n",
    "# 缺点：涉及到指数计算，计算量大；容易梯度消失；不关于原点对称\n",
    "# sigmoid函数处理负数\n",
    "print \"anwser 1:\", tf.nn.sigmoid(-2.9).eval()\n",
    "# sigmoid函数处理正数\n",
    "print \"anwser 2:\", tf.nn.sigmoid(3.4).eval()\n",
    "\n",
    "# 对a使用sigmoid函数进行激活处理，将结果保存到b中\n",
    "b = tf.nn.sigmoid(a)\n",
    "\n",
    "# 输出处理后的a，即b的值\n",
    "print \"sigmoid函数激活后的矩阵:\\n\", b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anwser 1: -0.9939633\nanwser 2: 0.9977749\ntanh函数激活后的矩阵:\n[[-0.7356101   0.16980994 -0.0637577  -0.98006594]\n [ 0.31066942 -0.60283226  0.7636776   0.97461796]\n [-0.42280796  0.5688813   0.1146892   0.3130676 ]\n [-0.79516643 -0.73749566 -0.9843301  -0.23153566]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# tanh激活函数：\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "# 解决sigmoid左右对称的问题\n",
    "# tanh函数处理负数\n",
    "print \"anwser 1:\", tf.nn.tanh(-2.9).eval()\n",
    "# tanh函数处理正数\n",
    "print \"anwser 2:\", tf.nn.tanh(3.4).eval()\n",
    "\n",
    "# 对a使用tanh函数进行激活处理，将结果保存到b中\n",
    "b = tf.nn.tanh(a)\n",
    "\n",
    "# 输出处理后的a，即b的值\n",
    "print \"tanh函数激活后的矩阵:\\n\", b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anwser 1: 0.0\nanwser 2: 3.4\nRelu函数激活后的矩阵:\n[[0.         0.17147096 0.         0.        ]\n [0.3212862  0.         1.0049797  2.177043  ]\n [0.         0.6458673  0.11519608 0.32394275]\n [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# Relu激活函数：max(0,x)\n",
    "# 解决sigmoid计算量大，梯度消失的问题，但在负数上梯度为0\n",
    "# Relu函数处理负数\n",
    "print \"anwser 1:\", tf.nn.relu(-2.9).eval()\n",
    "# Relu函数处理正数\n",
    "print \"anwser 2:\", tf.nn.relu(3.4).eval()\n",
    "\n",
    "# 对a使用Relu函数进行激活处理，将结果保存到b中\n",
    "b = tf.nn.relu(a)\n",
    "\n",
    "# 输出处理后的a，即b的值\n",
    "print \"Relu函数激活后的矩阵:\\n\", b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6974885  -0.65892005  0.23269276  1.010489  ]\n [ 1.8564698  -0.915025   -0.9290937   1.0934621 ]\n [ 0.9696932   0.77226484  1.6061519   1.3483294 ]\n [ 0.90551883 -0.05563325  0.12308426  0.3264205 ]]\n[[0.         0.         0.23269276 1.010489   1.1956359  1.0756383\n  0.         0.        ]\n [1.8564698  0.         0.         1.0934621  0.         2.4653983\n  2.6463966  0.        ]\n [0.9696932  0.77226484 1.6061519  1.3483294  0.         0.\n  0.         0.        ]\n [0.90551883 0.         0.12308426 0.3264205  0.         0.05724066\n  0.         0.        ]]\n[[0.         0.         0.23269276 1.010489  ]\n [1.8564698  0.         0.         1.0934621 ]\n [0.9696932  0.77226484 1.6061519  1.3483294 ]\n [0.90551883 0.         0.12308426 0.3264205 ]]\n[[0.2642943  0.29347524 0.81624657 1.3209406 ]\n [2.0016286  0.08155695 0.06850529 1.3824342 ]\n [1.2911963  1.1520467  1.7890218  1.5791821 ]\n [1.2450806  0.6649363  0.75658184 0.86961746]]\n[[-0.54455113 -0.5182205   0.18876785  0.50260854]\n [ 0.64991754 -0.7114329  -0.72575665  0.5223224 ]\n [ 0.4923067   0.43575025  0.6162925   0.57416534]\n [ 0.47520855 -0.05414157  0.10959485  0.24609125]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# elu:`exp(x) - 1` if < 0, `x` otherwise.\n",
    "print tf.nn.elu(a).eval()\n",
    "# [relu(x), relu(-x)]\n",
    "print tf.nn.crelu(a).eval()\n",
    "# min(max(x, 0), 6)\n",
    "print tf.nn.relu6(a).eval()\n",
    "# log(exp(x) + 1)\n",
    "print tf.nn.softplus(a).eval()\n",
    "# x / (abs(x) + 1)\n",
    "print tf.nn.softsign(a).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}